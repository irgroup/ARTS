{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "from utils import Text, apply_history\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text data\n",
    "num_texts = 94\n",
    "\n",
    "data = pickle.load(open(f\"/workspace/data/ARTS_only_texts_{num_texts}.pkl\", \"rb\"))\n",
    "determined_pairs = pickle.load(open(f\"/workspace/data/determined_pairs_{num_texts*24}.pkl\", \"rb\"))\n",
    "\n",
    "texts = {t_id : Text(t_id, text[0]) for t_id, text in data.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"/workspace/.env\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "I'm going to present you with two texts and I want you to decide which one is simpler. \n",
    "       The following guidelines should be taken into account for the decision: \n",
    "       Imagine you are writing an exam where you are allowed to google and where the task is to understand the two given texts.\n",
    "       Which of the two texts: generates less cognitive load?, can you understand more quickly?, are you more confident to answer questions about?, is easier for you to reformulate without changing the meaning?\n",
    "       Both Texts are delimited by ```\n",
    "\n",
    "       Text A:\n",
    "       ```\n",
    "       {TEXT_A}\n",
    "       ```\n",
    "\n",
    "       Text B: \n",
    "       ```\n",
    "       {TEXT_B}\n",
    "       ```\n",
    "\n",
    "       The answer should be either A or B, depending on which of the texts is easier to understand. \n",
    "       Please answer without any further text, just one letter.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4-1106-preview\"\n",
    "#model_name = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperature = 0 to suppress creativity\n",
    "\n",
    "prompt= PromptTemplate(template=prompt_template, input_variables=[\"TEXT_A\", \"TEXT_B\"])\n",
    "model = ChatOpenAI(temperature=0, model=model_name)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen_history(history):\n",
    "    gpt_history = manager.dict()\n",
    "    procs = []\n",
    "\n",
    "    for key, val in history.items():\n",
    "        if val[1] == -1:\n",
    "            i_a, i_b = determined_pairs[key]\n",
    "            proc = Process(target=handle_request, args=(i_a, i_b, texts, gpt_history, key))\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "        else:\n",
    "            gpt_history[key] = input_variables\n",
    "\n",
    "    for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "    return dict(gpt_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen_coin_flip(history):\n",
    "    for key, val in history.items():\n",
    "        if val[1] == -1:\n",
    "            entry = val\n",
    "            entry = (entry[0], entry[0][random.randint(0,1)], entry[2])\n",
    "            history[key] = entry\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_broken_history(history, verbose=False):\n",
    "    cnt = 0\n",
    "    for key, val in history.items():\n",
    "        if val[1] == -1:\n",
    "            cnt+= 1\n",
    "            if verbose:\n",
    "                i_a, i_b = determined_pairs[key]\n",
    "                print(i_a, i_b)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_request(i_a, i_b, texts, gpt_history, match_id):\n",
    "    system_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "    a = texts[i_a].get_text()\n",
    "    b = texts[i_b].get_text()\n",
    "    res = chain.invoke({\"TEXT_A\": a, \"TEXT_B\":  b})\n",
    "    winner = i_a\n",
    "    if res.lower() == 'a':\n",
    "        winner = i_b\n",
    "    elif res.lower() == 'b':\n",
    "        winner = i_a\n",
    "    else:\n",
    "        winner = -1\n",
    "    \n",
    "    entry = ((i_a, i_b), winner, system_time)\n",
    "    gpt_history[match_id] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 30\n",
    "procs = []\n",
    "\n",
    "\n",
    "gpt_history = manager.dict()\n",
    "open_indices = list(range(len(determined_pairs)))\n",
    "\n",
    "with tqdm(total = len(determined_pairs)) as pbar:\n",
    "    while len(open_indices) > 0:\n",
    "        for _ in range(num_processes):\n",
    "            if len(open_indices) <= 0:\n",
    "                continue\n",
    "            current_index = open_indices.pop()\n",
    "            i_a, i_b = determined_pairs[current_index]\n",
    "            proc = Process(target=handle_request, args=(i_a, i_b, texts, gpt_history, current_index))\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "            pbar.update(1)\n",
    "        \n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "\n",
    "\n",
    "gpt_history = dict(gpt_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load history\n",
    "gpt_history = pickle.load(open(f\"/workspace/{model_name}-{num_texts}_history.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved history at /workspace/gpt-4-1106-preview-94_2_history.pkl\n"
     ]
    }
   ],
   "source": [
    "path = f\"/workspace/{model_name}-{num_texts}_2_history.pkl\"\n",
    "pickle.dump(gpt_history, open(path, \"wb\"))\n",
    "print(f\"saved history at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
